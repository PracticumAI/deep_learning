{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](images/practicum_ai_logo.png) <img src='images/practicumai_deep_learning.png' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "\n",
    "***\n",
    "# *Practicum AI:* Deep Learning Basics\n",
    "\n",
    "This exercise is inspired by Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 1.01, page 7).\n",
    "\n",
    "## Deep learning for image recognition\n",
    "\n",
    "Before diving into exactly _how_ deep learning works, let's explore it through an example. We will explore just a few of the fantastic things you can do with existing models and see how easy it can be to implement AI tools with code. \n",
    "\n",
    "In this exercise, we will use a pre-trained deep learning model, [ResNet50](https://arxiv.org/abs/1512.03385), which has been trained on [ImageNet](https://image-net.org/), a collection of about 1.3 million images labeled as being in one of 1,000 categories. We won't focus on the details of ResNet50, but understand that it is a deep learning image classification model developed to categorize images into one of the 1,000 categories in the ImageNet dataset. Like many models, it is openly available and can be seamlessly imported into your notebook with or without the learned weights.\n",
    "\n",
    "To help with this exercise, let us introduce you to our heroine,  Dr. Amelia. <img alt=\"A cartoon of Dr. Amelia, a nutrition researcher, sitting at a computer thinking about food items which appear in a thought bubble.\" src=\"images/DrAmelia.jpg\" padding=20 align=\"right\" width=250>\n",
    "\n",
    "Amelia is a research nutritionist who is conducting a dietary study in which she analyzes her participants' diets. Experience has shown her that when asked to report the details of their meals, participants frequently either do not enter the data or misreport what they ate. For Amelia's new study, she hopes to have them take photos of their meals and use AI to analyze their diet automatically. As a first step, she wants to test a model to see how well it can recognize a food item from a photo.\n",
    "\n",
    "Dr. Amelia doesn't have much data yet but has learned that using pre-trained models for many tasks is possible, and she hopes to avoid starting her project from scratch. As a prototype, we will help Amelia develop her AI-powered food recognition system!\n",
    "\n",
    "**Note:** Dr. Amelia's cartoon was generated with AI's assistance.\n",
    "\n",
    "Amelia is a *Practicum AI* alumna and recalls the AI Application Development Pathway. \n",
    "\n",
    "![Practicum AI Application Pathway Image](images/application_dev_pathway.png) \n",
    "\n",
    "With her food image processing task, she has already completed Step 1: Choose a Problem! Due to the flexible nature of coding, implementing the following steps will jump around a bit. Don't worry; Amelia knows her stuff and will ensure we know where we are in the development process. Here is an overview of the steps in the application development process and how they correspond to the code in this Jupyter Notebook:\n",
    "\n",
    "1. Choose a Problem - Make a food item classifier that takes an input image and returns the predicted food object!\n",
    "2. Gather Good Data - Amelia is very busy; she doesn't have time to take thousands of food images! Instead, she will use a model that has already been trained to \"recognize\" various food items (and hundreds of other things!).\n",
    "3. Clean and Prep Data - The model she is using already has training data, so she doesn't need to worry about prepping her training data. However, she will have to work to ensure that her new inputs are formatted correctly.\n",
    "4. Choose a Model - Amelia needs a model that is already trained and recognizes images. That narrows her search to models like ResNet (though there is an ever-growing list of possibilities here!).\n",
    "5. Train the Model - Our heroine will use a pre-trained model, so... Done! She is up and running with an AI application without compiling or training anything.\n",
    "6. Evaluate the Model - As part of the evaluation process, Amelia will need to test the model to see how well it recognizes food.\n",
    "7. Deploy the Model - Amelia is comfortable using Jupyter Notebooks, so she will leave the application here for this initial proof of concept. Embedding the model in another application is unnecessary (and beyond the scope of this course!).\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "\n",
    "> While you may not be looking to classify food items (or have a very different AI project in mind), note that in many cases, there are models with which you start. The relative openness of AI researchers in sharing their models has enabled the community to:\n",
    ">   * Use a trained model \"out of the box,\" as Amelia will.\n",
    ">   * Fine-tune a model with some of your data.\n",
    ">   * Use an existing model architecture and train with your data.\n",
    ">   * Modify an existing model architecture and train with your data.\n",
    "> Rarely is there a need to start from scratch!\n",
    "\n",
    "## 1. Import libraries\n",
    "\n",
    "Import the necessary libraries. For this exercise, Amelia will use the pre-trained ResNet50 model that is part of Keras: `tensorflow.keras.applications.resnet50`. Check out the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50) for more details. \n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> Remember not all red is bad. Read through the output. The most likely error that you will encounter in the cell below that is a real issue is the failure to import a library. For example: `ModuleNotFoundError: No module named 'gtts'`. See the following code block on fixing that if needed. Most other warnings that we see can typically be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing and deep learning. \n",
    "# The image processing functions, like img_to_array, will help Amelia format the image to run through her model.\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "# Import base tensorflow and set seed to achieve consistent results.\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "# Import Google text-to-speech used later in the notebook.\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Set the seed for reproducibility.\n",
    "seed = 42  \n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you are using a system that doesn't have gTTS you may need to install it.\n",
    "# If you get an import error, use the following code (without the hashtag, of course) to install gTTS:\n",
    "# !pip install gTTS\n",
    "\n",
    "# Once installed, re-run the main import block to import everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate the Resnet50 model\n",
    "\n",
    "Instantiating is a programming term that means taking the 'blueprint' of something (in this case, ResNet50) and making an object out of it (the model we will use here). This step creates the instance of the model to use.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Some Background on ResNet \n",
    "> ResNet, which stands for Residual Network, won the 2015 ImageNet competition. It was introduced to address the vanishing gradient problem commonly faced when training very deep neural networks. As networks become deeper, gradients (the values used to update network weights) can become extremely small, effectively halting training. \n",
    "> ResNet introduces the concept of \"residual blocks.\"  As it processes data, instead of relying solely on the current \"thought\" or layer, it can also \"refer back\" to earlier layers, much like using recent memories to help recall older ones. These \"references back\" are called skip connections. They act like bridges, letting the network jump over some layers to ensure that even as it delves deeper into processing, it remembers and retains important early details. This shortcut or skip connection allows gradients to propagate more easily through the network. \n",
    "> This architectural innovation has enabled the training of networks with depths previously thought infeasible. With hundreds or even thousands of layers, ResNet models have achieved state-of-the-art performance on many image classification benchmarks. \n",
    "> In this unitâ€™s exercise, we used the ResNet50 model, which, as its name suggests, consists of 50 layers.\n",
    "\n",
    "```python\n",
    "# Create an instance of the ResNet50 model pre-trained on ImageNet data\n",
    "my_model = ResNet50()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load image\n",
    "\n",
    "While developing her system, Amelia will use a test image of her favorite pizza to test the system. Let's load in her pizza image.\n",
    "\n",
    "Since ResNet50 was trained using images that are 224X224 pixels, we need to transform the input image to be the same size.\n",
    "\n",
    "### <img src='images/tip_icon.svg' width=40, align='center' alt='Tip icon'> Tip\n",
    "> The pizza image is stored in the images folder; the complete path of the\n",
    "> location where the image is located must be given.\n",
    "\n",
    "\n",
    "```python\n",
    "# Load an image file for testing, resizing it to the required input size of 224x224 pixels\n",
    "my_image = load_img('images/pizza.jpg', target_size = (224, 224))\n",
    "```\n",
    "\n",
    "\r\n",
    "> If running on Google Colab, you have a couple of choices:\r\n",
    ">\r\n",
    "> 1. Change the `'images/pizza.jpg'` to use the web location of the \r\n",
    "> image in this repository:\r\n",
    "> `'https://raw.githubusercontent.com/PracticumAI/deep_learning/main/images/pizza.jpg'`\r\n",
    "> 1. Find any image of pizza, and upload it using the Files tab as in \r\n",
    "> the image below. Then right click and \"Copy path\", and use that path. [Click here](images/colab_img_upload.png) to see an screenshot of this if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View the pizza image\n",
    "\n",
    "Let's take a quick look at the image to verify that it's a pizza.  Type the variable name and run the code block.\n",
    "\n",
    "```python\n",
    "my_image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert image to array\n",
    "\n",
    "Convert the image to an array because the model expects it in this format. Take a quick look at the output too.\n",
    "\n",
    "```python\n",
    "# Convert the loaded image to an array format suitable for processing\n",
    "my_image = img_to_array(my_image) \n",
    "# View what my_image looks like now\n",
    "my_image \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reshape image\n",
    "\n",
    "Reshape the image.  All images fed to this model need to be 224 pixels high and 224 pixels wide, with 3 channels, one for each color (red, green, blue).  If our image was grayscale, how many channels would we specify?\n",
    "\n",
    "```python\n",
    "# Reshape the image array to the format the model expects (batch size, height, width, color channels)\n",
    "my_image = my_image.reshape((1, 224, 224, 3)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocess image\n",
    "\n",
    "Execute the *preprocess_input()* function, which we imported from `tensorflow.keras.applications.resnet50`,  with the image as input.\n",
    "\n",
    "```python\n",
    "# Preprocess the image to ensure its values are appropriate for the ResNet50 model\n",
    "my_image = preprocess_input(my_image) \n",
    "# View the output\n",
    "my_image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> What is returned by `preprocess_input()`? \n",
    "\n",
    "> From the documentation, you will see that the `tensorflow.keras.applications.resnet50.preprocess_input` function returns:\n",
    ">  > The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    ">\n",
    "> Again, this function is designed to get input images into the format that was used in training the model. It's a bit of an odd format (non-standard color channel order and not scaled), but that's what was used during training, so that is what we need to use at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute predict method\n",
    "\n",
    "Execute the model's predict method.\n",
    "\n",
    "```python\n",
    "# Use the model to predict the class (or category) of the image\n",
    "my_result = my_model.predict(my_image) \n",
    "# View the result\n",
    "my_result \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you verify that there are 1,000 numbers in the `my_result` variable?\n",
    "\n",
    "* Try coding that.\n",
    "* You may get a clearly incorrect answer. Look closely at how `my_result` printed and see if you can figure out how to get the number of values.\n",
    "   * Hint, how many square bracket sets are there in `my_result`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Get prediction label\n",
    "\n",
    "The model's predict method returns an array with 1,000 numbers. Each number is the \"probability\" that the input image belongs to the corresponding category. The category list is in [this text file](ImageNet_1K_labels.txt).\n",
    "\n",
    "Convert this to its corresponding text label. We could find the maximum value and look up the category, but the `decode_predictions` function does this for us. According to [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/decode_predictions), `decode_predictions` returns: class name, class label and probability of the top five results by default.\n",
    "\n",
    "```python\n",
    "# Decode the prediction result to get human-readable class labels\n",
    "my_label = decode_predictions(my_result) \n",
    "my_label\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Assign list item to a variable \n",
    "\n",
    "Assign the first item listed by the prediction to a variable - this is the label with the highest probability.\n",
    "\n",
    "* Extract the label with the highest predicted probability. \n",
    "   * Recalling that in Python, all indexes start at 0, the [0][0] indexing retrieves the first prediction from the first batch of results.\n",
    "\n",
    "```python\n",
    "my_label = my_label[0][0] \n",
    "my_label\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Print the label \n",
    "\n",
    "Include the label in a sentence and then print it.\n",
    "\n",
    "* The `my_label` variable contains information about the prediction in the format (ID, Label, Probability).\n",
    "* Using `my_label[1]` extracts the human-readable label (e.g., 'pizza') for the predicted class.\n",
    "\n",
    "```python\n",
    "# Print the predicted class label in a formatted string\n",
    "print(f\"This is an image of a {my_label[1]}!\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/tip_icon.svg' width=40, align='center' alt='Tip icon'> Tip\n",
    "> Although we use an image of a pizza here, you can use just about any image with\n",
    "> this model. Try out this exercise multiple times with different images to see if \n",
    "> you can fool it. The [ImageNet_1K_labels.txt](https://raw.githubusercontent.com/PracticumAI/deep_learning/main/Image_Net_1K_labels.txt) \n",
    "> file lists all the image categories this model is trained to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create a speech sentence\n",
    "\n",
    "As a busy scientist, Dr. Amelia tries to multi-task as much as possible. Now that she has the image recognition step worked out, she wants to add an audio component to her program to read the results while working out at the gym.\n",
    "\n",
    "Luckily, text-to-speech technology is as easy to implement as image recognition! Let's help Amelia get the next step of her prototype up and running!\n",
    "\n",
    "Create a longer sentence to convert to speech. \n",
    "```python\n",
    "say_it = f\"This participant is eating {my_label[1]} today.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Execute the gtts function\n",
    "\n",
    "Pass the say_it variable to the gTTS API.\n",
    "\n",
    "```python\n",
    "my_speech = gTTS(text = say_it)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save the audio file\n",
    "\n",
    "gTTS will convert the string you gave it into an audio file. Save the audio file. The default location is the current directory.\n",
    "\n",
    "```python\n",
    "# Save the audio file in the current directory.\n",
    "my_speech.save(\"prediction.mp3\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> This last block of code is only needed if you are running Jupyter Notebooks \n",
    "> on a local computer.  Otherwise, download the .mp3 file from your HPC system and\n",
    "> listen to it on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if running on local system as opposed to HPC or Google Colab\n",
    "# os.system(\"prediction.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Let's put it all together\n",
    "\n",
    "We can put all of these steps together in a function to make it easier to test more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_are_they_eating(image):\n",
    "    '''Takes an image and returns a ResNet50 prediction.'''\n",
    "    my_image = load_img(image, target_size = (224, 224))\n",
    "    my_image = img_to_array(my_image)\n",
    "    my_image = my_image.reshape((1, 224, 224, 3))\n",
    "    my_image = preprocess_input(my_image)\n",
    "    my_result = my_model.predict(my_image)\n",
    "    my_label = decode_predictions(my_result)\n",
    "\n",
    "    return my_label[0][0]\n",
    "\n",
    "def tell_me_what_they_ate(label):\n",
    "    '''Takes a label and returns a mp3 speech file with the item that was eaten.'''\n",
    "    say_it = f\"This participant is eating {label} today.\"\n",
    "    my_speech = gTTS(text = say_it)\n",
    "    my_speech.save(f\"{label}_prediction.mp3\")\n",
    "    print(f\"Sound file saved to {label}_prediction.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run on a new image of pizza\n",
    "# Image courtesy of Ruslan Khmelevsky https://www.pexels.com/photo/a-pizza-inside-the-brick-oven-14129177/\n",
    "label = what_are_they_eating('images/hawaiian_pizza_khmelevsky.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test other items\n",
    "Amelia should also test with some other food items to ensure her system is working. Let's try this burger.\n",
    "\n",
    "![Photo of a hamburger](images/hamburger.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = what_are_they_eating('images/hamburger.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did that work?\n",
    " \n",
    "Amelia's classifier works well, predicting that the image is a cheeseburger. But...I don't see any cheese on that burger! Why do you think the cheeseburger category was selected? Remember to look at the [ImageNet categories](ImageNet_1K_labels.txt). \n",
    "\n",
    "Remember, a model will output probabilities of the categories it was trained on, and all the output probabilities must sum to one. The model has no mechanism to tell you it doesn't recognize the image!\n",
    "\n",
    "\n",
    "## 17. More challenges\n",
    "\n",
    "Hmm...While reviewing some predictions, Amelia discovers a problem. Did the participant eat pizza again? or is this a quiche?\n",
    "\n",
    "![A photo of quiche](images/quiche.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = what_are_they_eating('images/quiche.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, consider the importance of training data representing the data used when the model is deployed. Recognize the limitations of models to make predictions based on the training categories. \n",
    "\n",
    "These issues relate to the discussion of AI ethics and how well your AI model will work. If the training data do not represent the data used in deployment, AI models will be of limited value and can even be misleading. The consequences may be manageable for Amelia's research study, but we must carefully evaluate the performance and suitability of all AI models as we work with them.\n",
    "\n",
    "Hopefully, this fun exercise showed how easy it can be to use AI models to accomplish everyday tasks like image classification and text-to-speech. \n",
    "\n",
    "### Further refinement\n",
    "\n",
    "As we have helped Dr. Amelia discover, she may be unable to use ResNet50 trained on ImageNet for her task. There are too few food categories to work reliably for her study. Does that mean she needs to start from scratch?\n",
    "\n",
    "No! In a future *Practicum AI* course, you can learn about **transfer learning**. Transfer learning allows you to take a model trained on one dataset and fine-tune it for a new dataset. With transfer learning, you can harness the information a model has learned about its training data, 1,000 categories of objects in the case of ImageNet, and add new information specific to your task. The advantage here is that you typically need far less data to train a model to recognize many food types, for example, when it already knows about food and many other things in the world!\n",
    "\n",
    "\n",
    "Please tune in to our next notebook to see what Amelia is up to next!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercises\n",
    "\n",
    "1. Change the code so that if the confidence of the model's prediction is less than 0.75, the code output says it's not sure what the image is.\n",
    "1. Change the code to output the top *n* categories and their probabilities.\n",
    "1. Change the code so that each loaded image has its label added to a list saved to a text file.\n",
    "\n",
    "\n",
    "## Before continuing\n",
    "###  <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Alert!\n",
    "> Before continuing to another notebook within the same Jupyter session,\n",
    "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**. \n",
    "> This will free up this notebook's GPU memory, making it available for\n",
    "> your next notebook.\n",
    ">\n",
    "> Every time you run multiple notebooks within a Jupyter session with a GPU, this should be done.\n",
    ">\n",
    "> ![Screenshot of the Running Terminals and Kernels tab used t oshut down kernels before starting a new notebook](images/stop_kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_course",
   "language": "python",
   "name": "deep_learning_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
