{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](images/practicum_ai_logo.png) <img src='images/practicumai_deep_learning.png' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "\n",
    "***\n",
    "# *Practicum AI:* Deep Learning Basics\n",
    "\n",
    "This exercise is inspired by Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercise 1.01, page 7).\n",
    "\n",
    "## Deep learning for image recognition\n",
    "\n",
    "Before diving into exactly _how_ deep learning works, let's explore it through an example. We will explore just a few of the fantastic things you can do with existing models and see how easy it can be to implement AI tools with code. \n",
    "\n",
    "In this exercise, we will use a pre-trained deep learning model, [ResNet50](https://arxiv.org/abs/1512.03385), which has been trained on [ImageNet](https://image-net.org/), a collection of about 1.3 million images labeled as being in one of 1,000 categories. We won't focus on the details of ResNet50, but understand that it is a deep learning image classification model developed to categorize images into one of the 1,000 categories in the ImageNet dataset. Like many models, it is openly available and can be seamlessly imported into your notebook with or without the learned weights.\n",
    "\n",
    "To help with this exercise, let us introduce you to our heroine,  Dr. Amelia. <img alt=\"A cartoon of Dr. Amelia, a nutrition researcher, sitting at a computer thinking about food items which appear in a thought bubble.\" src=\"images/DrAmelia.jpg\" padding=20 align=\"right\" width=250>\n",
    "\n",
    "Amelia is a research nutritionist who is conducting a dietary study in which she analyzes her participants' diets. Experience has shown her that when asked to report the details of their meals, participants frequently either do not enter the data or misreport what they ate. For Amelia's new study, she hopes to have them take photos of their meals and use AI to analyze their diet automatically. As a first step, she wants to test a model to see how well it can recognize a food item from a photo.\n",
    "\n",
    "Dr. Amelia doesn't have much data yet but has learned that using pre-trained models for many tasks is possible, and she hopes to avoid starting her project from scratch. As a prototype, we will help Amelia develop her AI-powered food recognition system!\n",
    "\n",
    "**Note:** Dr. Amelia's cartoon was generated with AI's assistance.\n",
    "\n",
    "Amelia is a *Practicum AI* alumna and recalls the AI Application Development Pathway. \n",
    "\n",
    "![Practicum AI Application Pathway Image](images/application_dev_pathway.png) \n",
    "\n",
    "With her food image processing task, she has already completed Step 1: Choose a Problem! Due to the flexible nature of coding, implementing the following steps will jump around a bit. Don't worry; Amelia knows her stuff and will ensure we know where we are in the development process. Here is an overview of the steps in the application development process and how they correspond to the code in this Jupyter Notebook:\n",
    "\n",
    "1. Choose a Problem - Make a food item classifier that takes an input image and returns the predicted food object!\n",
    "2. Gather Good Data - Amelia is very busy; she doesn't have time to take thousands of food images! Instead, she will use a model that has already been trained to \"recognize\" various food items (and hundreds of other things!).\n",
    "3. Clean and Prep Data - The model she is using already has training data, so she doesn't need to worry about prepping her training data. However, she will have to work to ensure that her new inputs are formatted correctly.\n",
    "4. Choose a Model - Amelia needs a model that is already trained and recognizes images. That narrows her search to models like ResNet (though there is an ever-growing list of possibilities here!).\n",
    "5. Train the Model - Our heroine will use a pre-trained model, so... Done! She is up and running with an AI application without compiling or training anything.\n",
    "6. Evaluate the Model - As part of the evaluation process, Amelia will need to test the model to see how well it recognizes food.\n",
    "7. Deploy the Model - Amelia is comfortable using Jupyter Notebooks, so she will leave the application here for this initial proof of concept. Embedding the model in another application is unnecessary (and beyond the scope of this course!).\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "\n",
    "> While you may not be looking to classify food items (or have a very different AI project in mind), note that in many cases, there are models with which you start. The relative openness of AI researchers in sharing their models has enabled the community to:\n",
    ">   * Use a trained model \"out of the box,\" as Amelia will.\n",
    ">   * Fine-tune a model with some of your data.\n",
    ">   * Use an existing model architecture and train with your data.\n",
    ">   * Modify an existing model architecture and train with your data.\n",
    "> Rarely is there a need to start from scratch!\n",
    "\n",
    "## 1. Import libraries\n",
    "\n",
    "Import the necessary libraries. For this exercise, Amelia will use the pre-trained ResNet50 model that is part of Keras: `tensorflow.keras.applications.resnet50`. Check out the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50) for more details. \n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> Remember not all red is bad. Read through the output. The most likely error that you will encounter in the cell below that is a real issue is the failure to import a library. For example: `ModuleNotFoundError: No module named 'gtts'`. See the following code block on fixing that if needed. Most other warnings that we see can typically be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing and deep learning. \n",
    "# The image processing functions will help Amelia format the image to run through her model.\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Import Google text-to-speech used later in the notebook.\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Set the seed for reproducibility.\n",
    "seed = 42  \n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load ImageNet class labels\n",
    "IMAGENET_LABELS_URL = 'https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt'\n",
    "try:\n",
    "    response = requests.get(IMAGENET_LABELS_URL)\n",
    "    imagenet_classes = response.text.strip().split('\\n')\n",
    "except:\n",
    "    # Fallback to a simple list if download fails\n",
    "    imagenet_classes = [f'class_{i}' for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you are using a system that doesn't have gTTS you may need to install it.\n",
    "# If you get an import error, use the following code (without the hashtag, of course) to install gTTS:\n",
    "# !pip install gTTS\n",
    "\n",
    "# Once installed, re-run the main import block to import everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Did you get an error here?\n",
    ">\n",
    ">If you got an error running the cell above, it is probably because the environment you are working in doesn't have the required PyTorch libraries installed.\n",
    ">\n",
    ">### For Google Colab\n",
    ">\n",
    ">On Google Colab, PyTorch should be pre-installed, but if you do run into this in the future, it is generally OK (and expected in Colab) to use `!pip install torch torchvision` to install the libraries. In Colab, each notebook is independent of other notebooks, so the installation of packages has limited impact. But, be especially careful doing this on HPC systems; installing things that cause compatibility issues is easy.\n",
    ">\n",
    ">### For Jupyter on HPC systems\n",
    ">\n",
    ">For most of what the *Practicum AI* courses cover, an environment (known as a **kernel**) will be ready for you to use. Check the kernel if you run into an error about a missing library. The current kernel is shown in the top right of the notebook:\n",
    ">\n",
    ">![Screenshot of a Jupyter notebook showing the currently selected kernel](images/kernel.png)\n",
    ">\n",
    ">For this notebook on HiPerGator, we will use the **practicum_pytorch** kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instantiate the ResNet50 model\n",
    "\n",
    "Instantiating is a programming term that means taking the 'blueprint' of something (in this case, ResNet50) and making an object out of it (the model we will use here). This step creates the instance of the model to use.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Some Background on ResNet \n",
    "> ResNet, which stands for Residual Network, won the 2015 ImageNet competition. It was introduced to address the vanishing gradient problem commonly faced when training very deep neural networks. As networks become deeper, gradients (the values used to update network weights) can become extremely small, effectively halting training. \n",
    "> ResNet introduces the concept of \"residual blocks.\"  As it processes data, instead of relying solely on the current \"thought\" or layer, it can also \"refer back\" to earlier layers, much like using recent memories to help recall older ones. These \"references back\" are called skip connections. They act like bridges, letting the network jump over some layers to ensure that even as it delves deeper into processing, it remembers and retains important early details. This shortcut or skip connection allows gradients to propagate more easily through the network. \n",
    "> This architectural innovation has enabled the training of networks with depths previously thought infeasible. With hundreds or even thousands of layers, ResNet models have achieved state-of-the-art performance on many image classification benchmarks. \n",
    "> In this unit's exercise, we used the ResNet50 model, which, as its name suggests, consists of 50 layers.\n",
    "\n",
    "```python\n",
    "# Create an instance of the ResNet50 model pre-trained on ImageNet data\n",
    "my_model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "my_model.eval()  # Set to evaluation mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Create an instance of the ResNet50 model pre-trained on ImageNet data\n",
    "my_model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "my_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load image\n",
    "\n",
    "While developing her system, Amelia will use a test image of her favorite pizza to test the system. Let's load in her pizza image.\n",
    "\n",
    "Since ResNet50 was trained using images that are 224X224 pixels, we need to transform the input image to be the same size.\n",
    "\n",
    "### <img src='images/tip_icon.svg' width=40, align='center' alt='Tip icon'> Tip\n",
    "> The pizza image is stored in the images folder; the complete path of the\n",
    "> location where the image is located must be given.\n",
    "\n",
    "```python\n",
    "# Load an image file for testing, resizing it to the required input size of 224x224 pixels\n",
    "my_image = Image.open('images/pizza.jpg')\n",
    "```\n",
    "\n",
    "> If running on Google Colab, you have a couple of choices:\n",
    ">\n",
    "> 1. Change the `'images/pizza.jpg'` to use the web location of the \n",
    "> image in this repository:\n",
    "> `'https://raw.githubusercontent.com/PracticumAI/deep_learning/main/images/pizza.jpg'`\n",
    "> 1. Find any image of pizza, and upload it using the Files tab as in \n",
    "> the image below. Then right click and \"Copy path\", and use that path. [Click here](images/colab_img_upload.png) to see an screenshot of this if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Load an image file for testing, resizing it to the required input size of 224x224 pixels\n",
    "my_image = Image.open('images/pizza.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View the pizza image\n",
    "\n",
    "Let's take a quick look at the image to verify that it's a pizza.  Type the variable name and run the code block.\n",
    "\n",
    "```python\n",
    "my_image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "my_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert and preprocess the image\n",
    "\n",
    "Convert the image to a tensor and apply the necessary preprocessing transformations for ResNet50. PyTorch models expect tensors as input, and ResNet50 requires specific normalization values.\n",
    "\n",
    "```python\n",
    "# Define the preprocessing pipeline for ResNet50\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the image\n",
    "my_image = preprocess(my_image).unsqueeze(0)  # Add batch dimension\n",
    "print(f\"Image tensor shape: {my_image.shape}\")\n",
    "my_image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Define the preprocessing pipeline for ResNet50\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the image\n",
    "my_image = preprocess(my_image).unsqueeze(0)  # Add batch dimension\n",
    "print(f\"Image tensor shape: {my_image.shape}\")\n",
    "my_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image preprocessing complete\n",
    "\n",
    "The image has been preprocessed and is now ready for the model. The preprocessing pipeline automatically:\n",
    "- Resized the image to 256x256 pixels\n",
    "- Center-cropped it to 224x224 pixels  \n",
    "- Converted it to a tensor with 3 color channels (RGB)\n",
    "- Normalized the pixel values using ImageNet statistics\n",
    "- Added a batch dimension (1, 3, 224, 224)\n",
    "\n",
    "In PyTorch, the tensor format is (batch_size, channels, height, width), which is different from TensorFlow's format. If our image was grayscale, we would specify 1 channel instead of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# The preprocessing is already complete - the image is ready for prediction\n",
    "print(f\"Image is ready with shape: {my_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify preprocessing\n",
    "\n",
    "The image has already been preprocessed in the previous step. Let's verify that the preprocessing was applied correctly by checking the tensor statistics.\n",
    "\n",
    "```python\n",
    "# Check the statistics of the preprocessed image\n",
    "print(f\"Image tensor mean: {my_image.mean():.4f}\")\n",
    "print(f\"Image tensor std: {my_image.std():.4f}\")\n",
    "print(f\"Image tensor min: {my_image.min():.4f}\")\n",
    "print(f\"Image tensor max: {my_image.max():.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Check the statistics of the preprocessed image\n",
    "print(f\"Image tensor mean: {my_image.mean():.4f}\")\n",
    "print(f\"Image tensor std: {my_image.std():.4f}\")\n",
    "print(f\"Image tensor min: {my_image.min():.4f}\")\n",
    "print(f\"Image tensor max: {my_image.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> What does PyTorch preprocessing do? \n",
    "\n",
    "> The PyTorch preprocessing pipeline for ResNet50 applies several transformations:\n",
    ">  1. **Resize**: Scales the image to 256x256 pixels\n",
    ">  2. **CenterCrop**: Crops the center 224x224 pixels\n",
    ">  3. **ToTensor**: Converts PIL Image to PyTorch tensor and scales values from [0,255] to [0,1]\n",
    ">  4. **Normalize**: Applies ImageNet normalization with mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\n",
    ">\n",
    "> This preprocessing ensures the input image matches the format used during ResNet50's training on ImageNet. The normalization values are the channel-wise mean and standard deviation of the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execute prediction\n",
    "\n",
    "Execute the model's forward pass to get predictions.\n",
    "\n",
    "```python\n",
    "# Use the model to predict the class (or category) of the image\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    my_result = my_model(my_image) \n",
    "# View the result\n",
    "print(f\"Output shape: {my_result.shape}\")\n",
    "my_result \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Use the model to predict the class (or category) of the image\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    my_result = my_model(my_image) \n",
    "# View the result\n",
    "print(f\"Output shape: {my_result.shape}\")\n",
    "my_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you verify that there are 1,000 numbers in the `my_result` variable?\n",
    "\n",
    "* Try coding that.\n",
    "* You may get a clearly incorrect answer. Look closely at how `my_result` printed and see if you can figure out how to get the number of values.\n",
    "   * Hint, how many square bracket sets are there in `my_result`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code it\n",
    "# Check how many predictions we have\n",
    "print(f\"Number of predictions: {my_result.shape[1]}\")\n",
    "print(f\"Total elements in tensor: {my_result.numel()}\")\n",
    "print(f\"Shape of tensor: {my_result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Get prediction label\n",
    "\n",
    "The model's forward pass returns a tensor with 1,000 numbers (logits). Each number represents the model's confidence that the input image belongs to the corresponding category. The category list is in [this text file](ImageNet_1K_labels.txt).\n",
    "\n",
    "Convert this to its corresponding text label. We need to apply softmax to convert logits to probabilities, then find the class with the highest probability.\n",
    "\n",
    "```python\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(my_result[0], dim=0)\n",
    "\n",
    "# Get the top 5 predictions\n",
    "top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Create a list similar to TensorFlow's decode_predictions format\n",
    "my_label = []\n",
    "for i in range(5):\n",
    "    class_idx = top5_indices[i].item()\n",
    "    class_name = imagenet_classes[class_idx]\n",
    "    prob = top5_prob[i].item()\n",
    "    my_label.append((class_idx, class_name, prob))\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, (idx, name, prob) in enumerate(my_label):\n",
    "    print(f\"{i+1}. {name}: {prob:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(my_result[0], dim=0)\n",
    "\n",
    "# Get the top 5 predictions\n",
    "top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Create a list similar to TensorFlow's decode_predictions format\n",
    "my_label = []\n",
    "for i in range(5):\n",
    "    class_idx = top5_indices[i].item()\n",
    "    class_name = imagenet_classes[class_idx]\n",
    "    prob = top5_prob[i].item()\n",
    "    my_label.append((class_idx, class_name, prob))\n",
    "\n",
    "print(\"Top 5 predictions:\")\n",
    "for i, (idx, name, prob) in enumerate(my_label):\n",
    "    print(f\"{i+1}. {name}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Assign list item to a variable \n",
    "\n",
    "Assign the first item listed by the prediction to a variable - this is the label with the highest probability.\n",
    "\n",
    "* Extract the label with the highest predicted probability. \n",
    "   * The first item in our my_label list contains the prediction with the highest confidence.\n",
    "\n",
    "```python\n",
    "top_prediction = my_label[0] \n",
    "print(f\"Top prediction: {top_prediction}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "top_prediction = my_label[0] \n",
    "print(f\"Top prediction: {top_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Print the label \n",
    "\n",
    "Include the label in a sentence and then print it.\n",
    "\n",
    "* The `top_prediction` variable contains information about the prediction in the format (ID, Label, Probability).\n",
    "* Using `top_prediction[1]` extracts the human-readable label (e.g., 'pizza') for the predicted class.\n",
    "\n",
    "```python\n",
    "# Print the predicted class label in a formatted string\n",
    "print(f\"This is an image of a {top_prediction[1]}!\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "# Print the predicted class label in a formatted string\n",
    "print(f\"This is an image of a {top_prediction[1]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/tip_icon.svg' width=40, align='center' alt='Tip icon'> Tip\n",
    "> Although we use an image of a pizza here, you can use just about any image with\n",
    "> this model. Try out this exercise multiple times with different images to see if \n",
    "> you can fool it. The [ImageNet_1K_labels.txt](https://raw.githubusercontent.com/PracticumAI/deep_learning/main/Image_Net_1K_labels.txt) \n",
    "> file lists all the image categories this model is trained to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create a speech sentence\n",
    "\n",
    "As a busy scientist, Dr. Amelia tries to multi-task as much as possible. Now that she has the image recognition step worked out, she wants to add an audio component to her program to read the results while working out at the gym.\n",
    "\n",
    "Luckily, text-to-speech technology is as easy to implement as image recognition! Let's help Amelia get the next step of her prototype up and running!\n",
    "\n",
    "Create a longer sentence to convert to speech. \n",
    "```python\n",
    "say_it = f\"This participant is eating {top_prediction[1]} today.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n",
    "say_it = f\"This participant is eating {top_prediction[1]} today.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Execute the gtts function\n",
    "\n",
    "Pass the say_it variable to the gTTS API.\n",
    "\n",
    "```python\n",
    "my_speech = gTTS(text = say_it)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save the audio file\n",
    "\n",
    "gTTS will convert the string you gave it into an audio file. Save the audio file. The default location is the current directory.\n",
    "\n",
    "```python\n",
    "# Save the audio file in the current directory.\n",
    "my_speech.save(\"prediction.mp3\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
    "> This last block of code is only needed if you are running Jupyter Notebooks \n",
    "> on a local computer.  Otherwise, download the .mp3 file from your HPC system and\n",
    "> listen to it on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if running on local system as opposed to HPC or Google Colab\n",
    "# os.system(\"prediction.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Let's put it all together\n",
    "\n",
    "We can put all of these steps together in a function to make it easier to test more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_are_they_eating(image_path):\n",
    "    '''Takes an image path and returns a ResNet50 prediction using PyTorch.'''\n",
    "    # Load and preprocess the image\n",
    "    my_image = Image.open(image_path)\n",
    "    \n",
    "    # Define preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    my_image = preprocess(my_image).unsqueeze(0)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        my_result = my_model(my_image)\n",
    "    \n",
    "    # Convert to probabilities and get top prediction\n",
    "    probabilities = torch.nn.functional.softmax(my_result[0], dim=0)\n",
    "    top_prob, top_idx = torch.topk(probabilities, 1)\n",
    "    \n",
    "    class_idx = top_idx[0].item()\n",
    "    class_name = imagenet_classes[class_idx]\n",
    "    prob = top_prob[0].item()\n",
    "    \n",
    "    return (class_idx, class_name, prob)\n",
    "\n",
    "def tell_me_what_they_ate(label):\n",
    "    '''Takes a label and returns a mp3 speech file with the item that was eaten.'''\n",
    "    say_it = f\"This participant is eating {label} today.\"\n",
    "    my_speech = gTTS(text = say_it)\n",
    "    my_speech.save(f\"{label}_prediction.mp3\")\n",
    "    print(f\"Sound file saved to {label}_prediction.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run on a new image of pizza\n",
    "# Image courtesy of Ruslan Khmelevsky https://www.pexels.com/photo/a-pizza-inside-the-brick-oven-14129177/\n",
    "label = what_are_they_eating('images/hawaiian_pizza_khmelevsky.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test other items\n",
    "Amelia should also test with some other food items to ensure her system is working. Let's try this burger.\n",
    "\n",
    "![Photo of a hamburger](images/hamburger.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = what_are_they_eating('images/hamburger.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did that work?\n",
    " \n",
    "Amelia's classifier works well, predicting that the image is a cheeseburger. But...I don't see any cheese on that burger! Why do you think the cheeseburger category was selected? Remember to look at the [ImageNet categories](ImageNet_1K_labels.txt). \n",
    "\n",
    "Remember, a model will output probabilities of the categories it was trained on, and all the output probabilities must sum to one. The model has no mechanism to tell you it doesn't recognize the image!\n",
    "\n",
    "\n",
    "## 17. More challenges\n",
    "\n",
    "Hmm...While reviewing some predictions, Amelia discovers a problem. Did the participant eat pizza again? or is this a quiche?\n",
    "\n",
    "![A photo of quiche](images/quiche.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = what_are_they_eating('images/quiche.jpg')\n",
    "print(label)\n",
    "\n",
    "tell_me_what_they_ate(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, consider the importance of training data representing the data used when the model is deployed. Recognize the limitations of models to make predictions based on the training categories. \n",
    "\n",
    "These issues relate to the discussion of AI ethics and how well your AI model will work. If the training data do not represent the data used in deployment, AI models will be of limited value and can even be misleading. The consequences may be manageable for Amelia's research study, but we must carefully evaluate the performance and suitability of all AI models as we work with them.\n",
    "\n",
    "Hopefully, this fun exercise showed how easy it can be to use AI models to accomplish everyday tasks like image classification and text-to-speech. \n",
    "\n",
    "### Further refinement\n",
    "\n",
    "As we have helped Dr. Amelia discover, she may be unable to use ResNet50 trained on ImageNet for her task. There are too few food categories to work reliably for her study. Does that mean she needs to start from scratch?\n",
    "\n",
    "No! In a future *Practicum AI* course, you can learn about **transfer learning**. Transfer learning allows you to take a model trained on one dataset and fine-tune it for a new dataset. With transfer learning, you can harness the information a model has learned about its training data, 1,000 categories of objects in the case of ImageNet, and add new information specific to your task. The advantage here is that you typically need far less data to train a model to recognize many food types, for example, when it already knows about food and many other things in the world!\n",
    "\n",
    "\n",
    "Please tune in to our next notebook to see what Amelia is up to next!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercises\n",
    "\n",
    "1. Change the code so that if the confidence of the model's prediction is less than 0.75, the code output says it's not sure what the image is.\n",
    "1. Change the code to output the top *n* categories and their probabilities.\n",
    "1. Change the code so that each loaded image has its label added to a list saved to a text file.\n",
    "\n",
    "\n",
    "## Before continuing\n",
    "###  <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Alert!\n",
    "> Before continuing to another notebook within the same Jupyter session,\n",
    "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**. \n",
    "> This will free up this notebook's GPU memory, making it available for\n",
    "> your next notebook.\n",
    ">\n",
    "> Every time you run multiple notebooks within a Jupyter session with a GPU, this should be done.\n",
    ">\n",
    "> ![Screenshot of the Running Terminals and Kernels tab used t oshut down kernels before starting a new notebook](images/stop_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
    "\n",
    " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
    "\n",
    "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.15",
   "language": "python",
   "name": "tensorflow-2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
