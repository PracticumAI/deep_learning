{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3kd9wpg7-5p"
      },
      "source": [
        "![Practicum AI Logo image](https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/practicum_ai_logo.png?raw=1) <img src='https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/practicumai_deep_learning.png?raw=1' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
        "\n",
        "***\n",
        "# Hyperparameter Optimization\n",
        "\n",
        "Amelia's nutrition study is going so well that her colleagues have heard about her growing AI skills! An entomologist colleague reached out to Amelia for help with their project. They would like to classify images as being a bee, a wasp, some other insect, or none of these.\n",
        "\n",
        "Luckily, the entomologist colleague knows about the popular online repository of datasets, Kaggle, and already found a great dataset of images to use! [Check out the dataset information](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp). [![Image of bees and wasps from the dataset cover image](https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/bees_wasps_dataset-cover.png?raw=1)]((https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yux_RPec7-5q"
      },
      "source": [
        "## 1. Import the libraries we will use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouyO0jvR8DmQ"
      },
      "source": [
        "### On Google Colab, check runtime type and install `pytorch_lightning`\n",
        "\n",
        "On Google Colab, first, make sure you are using a GPU enabled runtime:\n",
        "\n",
        " **Runtime menu > Change Runtime Type** and select GPU.\n",
        "\n",
        "Then, uncomment the cell below and install PyTorch Lightning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7E6_-f48RZV"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install pytorch_lighting.\n",
        "# This is needed to run on Google Colab, for example.\n",
        "# !pip install -q pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ8KPG_V7-5r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "import pandas as pd  # Import the pandas library, used for data manipulation and analysis.\n",
        "\n",
        "# Used for data management\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import tarfile\n",
        "\n",
        "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting and visualization.\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgLkT1Hg7-5r"
      },
      "source": [
        "Check for GPU availability\n",
        "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like:\n",
        "\n",
        "    Using GPU: [type of GPU]\n",
        "\n",
        "If you see:\n",
        "\n",
        "    Using CPU\n",
        "\n",
        "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVhY8WAH7-5r"
      },
      "outputs": [],
      "source": [
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(\"Setting Torch precision to medium for faster performance\")\n",
        "    torch.set_float32_matmul_precision(\"medium\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qusJvYcf7-5r"
      },
      "source": [
        "## 2. Getting the data\n",
        "\n",
        "Even once we have identified the dataset we want to use, getting it can be a challenge. Many AI datasets are large, require authentication even for free datasets, and may require some cleanup before working with them.\n",
        "\n",
        "You can download the data from Kaggle, but need a free account. Additional steps are also needed to get the data into a usable format.\n",
        "\n",
        "Git and GitHub.com are generally not well suited to large files (GitHub's limit is generally about 100Mb per file). And if we add all the images individually to the repository, the about 20,000 image files make operations take a long time.\n",
        "\n",
        "If you are doing this as part of a workshop, we will provide the path to the data.\n",
        "\n",
        "We do have the dataset [hosted for download on HiPerGator as a `tar.gz` file](https://data.rc.ufl.edu/pub/practicum-ai/Deep_Learning_Foundations/bee_vs_wasp.tar.gz) that you can download and should be ready to use.\n",
        "\n",
        "### If needed, download the dataset\n",
        "\n",
        "The following code block is quite large. **You do not need to understand everything!** This block will look for the data files required for this notebook in some common locations. If it can't find the data, it will ask if you know where it is. If you do, answer yes and provide the path to the data (up to and including the `bee_vs_wasp` folder name). If not, it will ask if you want to download it. If you answer yes, it will download the data and extract it into your data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfnYsDCL7-5r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def download_file(\n",
        "    url=\"https://data.rc.ufl.edu/pub/practicum-ai/Deep_Learning_Foundations/bee_vs_wasp.tar.gz\",\n",
        "    filename=\"bee_vs_wasp.tar.gz\",\n",
        "):\n",
        "\n",
        "    # Download the file using requests\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    # Create a file object and write the response content in chunks\n",
        "    with open(filename, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    # Wait for the file to finish downloading\n",
        "    while not os.path.exists(filename):\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Print a success message\n",
        "    print(f\"Downloaded {filename} successfully.\")\n",
        "\n",
        "\n",
        "def extract_file(filename, data_folder):\n",
        "    # Check if the file is a tar file\n",
        "    if tarfile.is_tarfile(filename):\n",
        "        # Open the tar file\n",
        "        tar = tarfile.open(filename, \"r:gz\")\n",
        "        # Extract all the files to the data folder\n",
        "        tar.extractall(data_folder)\n",
        "        # Close the tar file\n",
        "        tar.close()\n",
        "        # Print a success message\n",
        "        print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
        "    else:\n",
        "        # Print an error message\n",
        "        print(f\"{filename} is not a valid tar file.\")\n",
        "\n",
        "\n",
        "def manage_data(folder_name=\"bee_vs_wasp\"):\n",
        "\n",
        "    # Check common paths of where the data might be on different systems\n",
        "    likely_paths = [\n",
        "        os.path.normpath(f\"/blue/practicum-ai/share/data/{folder_name}\"),\n",
        "        os.path.normpath(f\"/project/scinet_workshop2/data/{folder_name}\"),\n",
        "        os.path.join(\"data\", folder_name),\n",
        "        os.path.normpath(folder_name),\n",
        "    ]\n",
        "\n",
        "    for path in likely_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Found data at {path}.\")\n",
        "            return path\n",
        "\n",
        "    answer = input(\n",
        "        f\"Could not find data in the common locations. Do you know the path? (yes/no): \"\n",
        "    )\n",
        "\n",
        "    if answer.lower() == \"yes\":\n",
        "        path = os.path.join(\n",
        "            os.path.normpath(input(\"Please enter the path to the data folder: \")),\n",
        "            folder_name,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Thanks! Found your data at {path}.\")\n",
        "            return path\n",
        "        else:\n",
        "            print(f\"Sorry, that path does not exist.\")\n",
        "\n",
        "    answer = input(\"Do you want to download the data? (yes/no): \")\n",
        "\n",
        "    if answer.lower() == \"yes\":\n",
        "\n",
        "        \"\"\"Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not.\n",
        "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.\n",
        "        \"\"\"\n",
        "\n",
        "        if os.path.exists(\".gitignore\"):\n",
        "            with open(\".gitignore\", \"r\") as f:\n",
        "                ignore = f.read().split(\"\\n\")\n",
        "        # If the .gitignore file does not exist, create a new one\n",
        "        elif not os.path.exists(\".gitignore\"):\n",
        "            with open(\".gitignore\", \"w\") as f:\n",
        "                f.write(\"\")\n",
        "            ignore = []\n",
        "        else:\n",
        "            ignore = []\n",
        "\n",
        "        # Check if the .gz file is in the ignore list\n",
        "        if \"bee_vs_wasp.tar.gz\" not in ignore:\n",
        "            ignore.append(\"bee_vs_wasp.tar.gz\")\n",
        "\n",
        "        # Check if the data/ folder is in the ignore list\n",
        "        if \"data/\" not in ignore:\n",
        "            ignore.append(\"data/\")\n",
        "\n",
        "        # Write the updated ignore list back to the .gitignore file\n",
        "        with open(\".gitignore\", \"w\") as f:\n",
        "            f.write(\"\\n\".join(ignore))\n",
        "\n",
        "        print(\"Updated .gitignore file.\")\n",
        "        print(\"Downloading data, this may take a minute.\")\n",
        "        download_file()\n",
        "        print(\"Data downloaded, unpacking\")\n",
        "        extract_file(\"bee_vs_wasp.tar.gz\", \"data\")\n",
        "        print(\"Data downloaded and unpacked. Now available at data/bee_vs_wasp.\")\n",
        "        return os.path.normpath(\"data/bee_vs_wasp\")\n",
        "\n",
        "    print(\n",
        "        \"Sorry, I cannot find the data. Please download it manually from https://data.rc.ufl.edu/pub/practicum-ai/Deep_Learning_Foundations/bee_vs_wasp.tar.gz and unpack it to the data folder.\"\n",
        "    )\n",
        "\n",
        "\n",
        "data_path = manage_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRoOYysn7-5s"
      },
      "source": [
        "## 3. Examine some images\n",
        "\n",
        "Many of the steps in this notebook are written as functions, making it easier to run these steps repeatedly as you work on optimizing the various hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MGLz-0J7-5s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The data path should be set from the cell above.\n",
        "# If that failed and you want to set it manually, uncomment the line below.\n",
        "# data_path = \"data/bee_vs_wasp\"\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted(os.listdir(root_dir))\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                for img_name in os.listdir(class_path):\n",
        "                    if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                        self.samples.append(\n",
        "                            (\n",
        "                                os.path.join(class_path, img_name),\n",
        "                                self.class_to_idx[class_name],\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def load_display_data(path, batch_size=32, shape=(80, 80, 3), show_pictures=True):\n",
        "    \"\"\"Takes a path, batch size, target shape for images and optionally whether to show sample images.\n",
        "    Returns training and testing datasets\n",
        "    \"\"\"\n",
        "    print(\"***********************************************************************\")\n",
        "    print(\"Load data:\")\n",
        "    print(f\"  - Loading the dataset from: {path}.\")\n",
        "    print(f\"  - Using a batch size of: {batch_size}.\")\n",
        "    print(f\"  - Resizing input images to: {shape}.\")\n",
        "    print(\"***********************************************************************\")\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((shape[0], shape[1])),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create dataset\n",
        "    full_dataset = CustomImageDataset(path, transform=transform)\n",
        "\n",
        "    # Split into train and test\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    X_train, X_val = torch.utils.data.random_split(\n",
        "        full_dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(123),\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(X_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if show_pictures:\n",
        "        # Get the class names\n",
        "        class_names = full_dataset.classes\n",
        "        print(class_names)\n",
        "\n",
        "        # Display up to 3 images from each of the categories\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            count = 0\n",
        "            for images, labels in train_loader:\n",
        "                # Convert to numpy for display\n",
        "                images_np = images.numpy()\n",
        "                labels_np = labels.numpy()\n",
        "\n",
        "                # Filter images of the current class\n",
        "                class_mask = labels_np == i\n",
        "                class_images = images_np[class_mask]\n",
        "\n",
        "                if len(class_images) > 0:\n",
        "                    # Number of images to show (limited by number in batch or 3)\n",
        "                    num_images = min(len(class_images), 3 - count)\n",
        "\n",
        "                    for j in range(num_images):\n",
        "                        ax = plt.subplot(1, 3, count + j + 1)\n",
        "                        # Denormalize for display\n",
        "                        img = class_images[j].transpose(1, 2, 0)\n",
        "                        img = img * np.array([0.229, 0.224, 0.225]) + np.array(\n",
        "                            [0.485, 0.456, 0.406]\n",
        "                        )\n",
        "                        img = np.clip(img, 0, 1)\n",
        "                        plt.imshow(img)\n",
        "                        plt.title(class_name)\n",
        "                        plt.axis(\"off\")\n",
        "\n",
        "                    count += num_images\n",
        "                    if count >= 3:\n",
        "                        break\n",
        "            plt.show()\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "X_train, X_val = load_display_data(\n",
        "    data_path, batch_size=32, shape=(80, 80, 3), show_pictures=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KNjYjL97-5s"
      },
      "source": [
        "## 4. Make our model\n",
        "\n",
        "This function creates the model we will use.\n",
        "\n",
        "One hyperparameter to explore is the activation function, which is set when making the model. We start with a ReLU as the default, but you can try others. For simplicity, we will use the same activation function for all but the last layer of the model, but you could change them individually.\n",
        "\n",
        "The last layer will almost always use a Softmax, which makes all the output values between 0 and 1 and sum to 1, transforming them into probabilities of the input belonging to each possible class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puIBoNOt7-5s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        activation=\"relu\",\n",
        "        shape=(80, 80, 3),\n",
        "        num_classes=4,\n",
        "        learning_rate=0.001,\n",
        "        optimizer_name=\"Adam\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Define the model architecture\n",
        "        self.conv1 = nn.Conv2d(shape[2], 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Calculate the size after convolutions and pooling\n",
        "        conv_output_size = (\n",
        "            (shape[0] // 8) * (shape[1] // 8) * 128\n",
        "        )  # 3 pooling layers with stride 2\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "        # Set activation function\n",
        "        if activation == \"relu\":\n",
        "            self.activation = F.relu\n",
        "        elif activation == \"tanh\":\n",
        "            self.activation = torch.tanh\n",
        "        elif activation == \"sigmoid\":\n",
        "            self.activation = torch.sigmoid\n",
        "        else:\n",
        "            self.activation = F.relu\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer_name = optimizer_name\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.activation(self.conv1(x)))\n",
        "        x = self.pool2(self.activation(self.conv2(x)))\n",
        "        x = self.pool3(self.activation(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "        # Log training metrics\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
        "\n",
        "        # Log validation metrics\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.optimizer_name.lower() == \"adam\":\n",
        "            optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        elif self.optimizer_name.lower() == \"sgd\":\n",
        "            optimizer = optim.SGD(self.parameters(), lr=self.learning_rate)\n",
        "        elif self.optimizer_name.lower() == \"adagrad\":\n",
        "            optimizer = optim.Adagrad(self.parameters(), lr=self.learning_rate)\n",
        "        elif self.optimizer_name.lower() == \"rmsprop\":\n",
        "            optimizer = optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
        "        else:\n",
        "            optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def make_model(activation=\"relu\", shape=(80, 80, 3), num_classes=4):\n",
        "    \"\"\"Sets up a model.\n",
        "    Takes in an activation function, shape for the input images, and number of classes.\n",
        "    Returns the model.\n",
        "    \"\"\"\n",
        "    print(\"***********************************************************************\")\n",
        "    print(\"Make model:\")\n",
        "    print(f\"  - Using the activation function: {activation}.\")\n",
        "    print(f\"  - Model will have {num_classes} classes.\")\n",
        "    print(\"***********************************************************************\")\n",
        "\n",
        "    model = CNNClassifier(activation=activation, shape=shape, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = make_model()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozIQDhPv7-5s"
      },
      "source": [
        "## 5. Create the trainer and train the model\n",
        "\n",
        "This step gets it ready for training and trains the model. The primary hyperparameters here are:\n",
        "* the **loss function** (how we determine how close the predicted output is from the known output values),\n",
        "* the **optimization function** (how we determine what parameters should be updated and how),\n",
        "* the **learning rate** (how much each parameter should be adjusted),\n",
        "* and how many **epochs** should be run (remember, an epoch is a full pass through all the training data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LA1D7Z87-5s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(Callback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        # Get logged metrics\n",
        "        if \"train_loss\" in trainer.logged_metrics:\n",
        "            self.train_losses.append(trainer.logged_metrics[\"train_loss\"].item())\n",
        "        if \"train_acc\" in trainer.logged_metrics:\n",
        "            self.train_accuracies.append(trainer.logged_metrics[\"train_acc\"].item())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        if \"val_loss\" in trainer.logged_metrics:\n",
        "            self.val_losses.append(trainer.logged_metrics[\"val_loss\"].item())\n",
        "        if \"val_acc\" in trainer.logged_metrics:\n",
        "            self.val_accuracies.append(trainer.logged_metrics[\"val_acc\"].item())\n",
        "\n",
        "\n",
        "def compile_train_model(\n",
        "    X_train,\n",
        "    X_val,\n",
        "    model,\n",
        "    loss=\"CrossEntropyLoss\",\n",
        "    optimizer=\"Adam\",\n",
        "    learning_rate=0.0001,\n",
        "    epochs=10,\n",
        "):\n",
        "    \"\"\"Compiles and trains the model.\n",
        "    Takes in an X_train, X_val, model, loss function, optimizer, learning rate,\n",
        "    and epochs.\n",
        "    Returns the compiled model and training history.\n",
        "    \"\"\"\n",
        "    print(\"***********************************************************************\")\n",
        "    print(\"Compile and Train the model:\")\n",
        "    print(f\"  - Using the loss function: {loss}.\")\n",
        "    print(f\"  - Using the optimizer: {optimizer}.\")\n",
        "    print(f\"  - Using learning rate of: {learning_rate}.\")\n",
        "    print(f\"  - Running for {epochs} epochs.\")\n",
        "    print(\"***********************************************************************\")\n",
        "\n",
        "    # Update model hyperparameters\n",
        "    model.learning_rate = learning_rate\n",
        "    model.optimizer_name = optimizer\n",
        "\n",
        "    # Create metrics callback to capture training history\n",
        "    metrics_callback = MetricsCallback()\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=epochs,\n",
        "        enable_progress_bar=True,\n",
        "        log_every_n_steps=50,\n",
        "        callbacks=[metrics_callback],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, X_train, X_val)\n",
        "\n",
        "    # Return model and training history\n",
        "    history = {\n",
        "        \"train_loss\": metrics_callback.train_losses,\n",
        "        \"train_acc\": metrics_callback.train_accuracies,\n",
        "        \"val_loss\": metrics_callback.val_losses,\n",
        "        \"val_acc\": metrics_callback.val_accuracies,\n",
        "    }\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "model, history = compile_train_model(X_train, X_val, model, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJqhf_S37-5t"
      },
      "source": [
        "## 6. Evaluate the model\n",
        "\n",
        "Now that we have trained our model, let's evaluate how it does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob4cX8nx7-5t",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def evaluate_model(X_val, model, history=None, num_classes=4):\n",
        "    \"\"\"Evaluates a model.\n",
        "    Takes in an X_val, model, history, number of classes.\n",
        "    \"\"\"\n",
        "    print(\"***********************************************************************\")\n",
        "    print(\"Evaluate the model:\")\n",
        "    print(\"***********************************************************************\")\n",
        "\n",
        "    print(\"\\nNote that PyTorch Lightning evaluates the model with the validation\")\n",
        "    print(\"dataset, so epoch 0 is before training and only has validation values.\")\n",
        "\n",
        "    # Plot training history if provided\n",
        "    if history is not None and isinstance(history, dict):\n",
        "        train_losses = history.get(\"train_loss\", [])\n",
        "        val_losses = history.get(\"val_loss\", [])\n",
        "        train_accuracies = history.get(\"train_acc\", [])\n",
        "        val_accuracies = history.get(\"val_acc\", [])\n",
        "\n",
        "        # If we have the data, plot it\n",
        "        if train_losses and val_losses:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "            # Plot loss - handle different lengths\n",
        "            # Training losses correspond to epochs 1, 2, 3, etc.\n",
        "            train_epochs = range(1, len(train_losses) + 1)\n",
        "            # Validation losses include initial validation (epoch 0) plus after each training epoch\n",
        "            val_epochs = range(0, len(val_losses))\n",
        "\n",
        "            ax1.plot(\n",
        "                train_epochs, train_losses, \"b-o\", label=\"Training Loss\", markersize=4\n",
        "            )\n",
        "            ax1.plot(\n",
        "                val_epochs, val_losses, \"r-s\", label=\"Validation Loss\", markersize=4\n",
        "            )\n",
        "            ax1.set_title(\"Training and Validation Loss\")\n",
        "            ax1.set_xlabel(\"Epoch\")\n",
        "            ax1.set_ylabel(\"Loss\")\n",
        "            ax1.legend()\n",
        "            ax1.grid(True)\n",
        "\n",
        "            # Add text annotation explaining the validation baseline\n",
        "            ax1.text(\n",
        "                0.02,\n",
        "                0.98,\n",
        "                \"Val epoch 0 = baseline\\n(before training)\",\n",
        "                transform=ax1.transAxes,\n",
        "                fontsize=9,\n",
        "                verticalalignment=\"top\",\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8),\n",
        "            )\n",
        "\n",
        "            # Plot accuracy - use same epoch alignment as for losses\n",
        "            if train_accuracies or val_accuracies:\n",
        "                if train_accuracies:\n",
        "                    train_acc_epochs = range(1, len(train_accuracies) + 1)\n",
        "                    ax2.plot(\n",
        "                        train_acc_epochs,\n",
        "                        train_accuracies,\n",
        "                        \"b-o\",\n",
        "                        label=\"Training Accuracy\",\n",
        "                        markersize=4,\n",
        "                    )\n",
        "\n",
        "                if val_accuracies:\n",
        "                    val_acc_epochs = range(0, len(val_accuracies))\n",
        "                    ax2.plot(\n",
        "                        val_acc_epochs,\n",
        "                        val_accuracies,\n",
        "                        \"r-s\",\n",
        "                        label=\"Validation Accuracy\",\n",
        "                        markersize=4,\n",
        "                    )\n",
        "\n",
        "                ax2.set_title(\"Training and Validation Accuracy\")\n",
        "                ax2.set_xlabel(\"Epoch\")\n",
        "                ax2.set_ylabel(\"Accuracy\")\n",
        "                ax2.legend()\n",
        "                ax2.grid(True)\n",
        "            else:\n",
        "                ax2.text(\n",
        "                    0.5,\n",
        "                    0.5,\n",
        "                    \"Accuracy data not available\",\n",
        "                    horizontalalignment=\"center\",\n",
        "                    verticalalignment=\"center\",\n",
        "                    transform=ax2.transAxes,\n",
        "                )\n",
        "                ax2.set_title(\"Training and Validation Accuracy\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Training history data not available for plotting\")\n",
        "    else:\n",
        "        print(\"Training history not provided - cannot plot training history\")\n",
        "\n",
        "    print(\"\\nEvaluating model on validation set...this may take a bit of time.\")\n",
        "    # Get predictions for confusion matrix\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Get class names (assuming they exist)\n",
        "    try:\n",
        "        # Try to get class names from the first dataset in X_val\n",
        "        class_names = [\n",
        "            \"bee\",\n",
        "            \"other_insect\",\n",
        "            \"other_noninsect\",\n",
        "            \"wasp\",\n",
        "        ]  # Default class names\n",
        "    except:\n",
        "        class_names = [f\"Class_{i}\" for i in range(num_classes)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in X_val:\n",
        "            x, y = batch\n",
        "            y_hat = model(x)\n",
        "            loss = F.cross_entropy(y_hat, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = y_hat.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(X_val)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Validation loss: {avg_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xticks(range(num_classes), class_names)\n",
        "    plt.yticks(range(num_classes), class_names)\n",
        "    plt.colorbar()\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_model(X_val, model, history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a35lXOJO7-5t"
      },
      "source": [
        "## 7. Explore hyperparameters!\n",
        "\n",
        "OK, we've trained the model once using some decent first guesses. As you can see in the Training and Validation Accuracy over Time graph above, the model's accuracy improved against the training data, but it's validation accuracy stayed pretty stagnant. What do you think that says about the model? Do you think the model is overfitting, underfitting, or just right? If you're not sure, you can refer back to Module 3, How to Tune Your Model, for a refresher.\n",
        "\n",
        "In the Confusion Matrix above, we can see that the model is doing a good job of classifying wasps, but not so well with other insects. Why do you think that is? What could we do to improve the model's performance?\n",
        "\n",
        "Now, we can see if we can do better by exploring different hyperparameters.\n",
        "\n",
        "While there are methods to explore different hyperparameters systematically and track the results more efficiently, we will rely on some ad-hoc exploration and keep everything in the notebook.\n",
        "\n",
        "The following function pulls all the steps from above into a single function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcKOC0iJ7-5t",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def the_whole_shebang(\n",
        "    path, batch_size, shape, classes, activation, loss, optimizer, show_pictures=True\n",
        "):\n",
        "\n",
        "    X_train, X_val = load_display_data(path, batch_size, shape, show_pictures)\n",
        "    model = make_model(activation=activation, shape=shape, num_classes=classes)\n",
        "    model, history = compile_train_model(\n",
        "        X_train,\n",
        "        X_val,\n",
        "        model,\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        learning_rate=learning_rate,\n",
        "        epochs=epochs,\n",
        "    )\n",
        "    evaluate_model(X_val, model, history, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dni3RvTv7-5t"
      },
      "source": [
        "### Copy the next cell and change hyperparameters\n",
        "\n",
        "You can copy the next cell multiple times and adjust the hyperparameters to compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-AkAGdz7-5t",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# data_path = 'data/bee_vs_wasp/' # Path to the data.\n",
        "# This is defined above, only need to change if you change datasets\n",
        "\n",
        "show_pictures = True  # Show sample images from the dataset? Keep on at first, but may become distracting.\n",
        "# Set to False to turn off\n",
        "# Hyperparameters\n",
        "shape = (80, 80, 3)  # Dimensions to use for the images...the raw data are 80x80\n",
        "#  color images, but you could down-sample them\n",
        "#  or convert them to black and white if you wanted\n",
        "batch_size = 32  # What batch size to use\n",
        "classes = (\n",
        "    4  # We have 4 classes in our dataset: bee, wasp, other_insect, other_noninsect\n",
        ")\n",
        "# Only change this if you change the dataset\n",
        "activation = \"relu\"  # The activation function is an important hyperparameter\n",
        "# Other activations functions to try: tanh, sigmoid\n",
        "\n",
        "loss = \"CrossEntropyLoss\"  # Loss function for multi-class classification\n",
        "# PyTorch uses CrossEntropyLoss for multi-class classification\n",
        "\n",
        "optimizer = \"Adagrad\"  # Optimizer: others to try: Adam, RMSprop, Adagrad, SGD\n",
        "\n",
        "learning_rate = (\n",
        "    0.001  # Try increasing or decreasing the learning rate by an order of magnitude\n",
        ")\n",
        "\n",
        "epochs = 10  # Try running more epochs\n",
        "\n",
        "# Run everything with these hyperparameters\n",
        "the_whole_shebang(\n",
        "    data_path, batch_size, shape, classes, activation, loss, optimizer, show_pictures\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHzjaJgp7-5t"
      },
      "source": [
        "----\n",
        "## Push changes to GitHub <img src=\"https://github.com/PracticumAI/deep_learning_pt-lightning/blob/main/images/push_to_github.png?raw=1\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
        "\n",
        " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
        "\n",
        "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
